# ğŸ“˜ KnowledgeHub AI

### Intelligent Document Intelligence System for GlobalLogicâ€“Hitachi

---

## 1. Introduction

**KnowledgeHub AI** is a local, privacy-focused **Retrieval-Augmented Generation (RAG)** based system designed to enable intelligent querying over internal company documents of **GlobalLogicâ€“Hitachi**.

Organizations typically store knowledge across large volumes of PDFs, Word documents, text files, and internal documentation. Searching such data manually is inefficient and time-consuming. KnowledgeHub AI addresses this problem by allowing users to ask questions in **natural language** and receive **accurate, context-aware answers** derived strictly from company documents.

The system is designed to be **modular, configurable, model-agnostic**, and **scalable**, supporting both local LLMs (via Ollama) and future cloud-based models.

---

## 2. Problem Statement

* Internal company knowledge is spread across multiple document formats.
* Traditional keyword-based search lacks semantic understanding.
* Manual document exploration wastes time and reduces productivity.
* Cloud-based AI solutions pose **privacy and cost concerns**.

---

## 3. Proposed Solution

KnowledgeHub AI uses a **Retrieval-Augmented Generation (RAG)** architecture that:

* Converts documents into semantic embeddings.
* Stores them in a vector database.
* Retrieves only the most relevant document segments.
* Uses a Large Language Model (LLM) to generate answers **only from retrieved context**, avoiding hallucination.

---

## 4. High-Level RAG Flow

### Data Ingestion Flow

```
Documents
â†’ Parsing
â†’ Chunking
â†’ Embedding
â†’ Vector Storage
```

### Runtime Query Flow

```
User Query
â†’ Query Embedding
â†’ Vector DB Similarity Search (Top-K)
â†’ Context Assembly
â†’ LLM Prompting
â†’ Final Answer
```

ğŸ“Œ No agent is required initially. Agentic behavior is planned in future versions.

EXPLANATIONS FOR ABOVE FLOWS:

# ğŸ” High-Level RAG Flow (Backend Working â€“ Simple Explanation)

Tumhare project me **do alag phases** hote hain:

1ï¸âƒ£ **Data Ingestion (Ek baar ya jab new files aayen)**
2ï¸âƒ£ **Runtime Query (Har user question pe)**

Main dono ko **step-by-step backend ke point of view se** samjhaata hoon.

---

## ğŸŸ¦ PART 1: Data Ingestion Flow

ğŸ‘‰ *Ye process tab hota hai jab tum documents add karti ho*
(example: GlobalLogicâ€“Hitachi PDFs, HR policies, guidelines)

### Step 1: Documents

ğŸ“‚ `data/` folder me files hoti hain:

* PDF
* Word
* TXT
* JSON

ğŸ’¡ Example:

```
data/
 â””â”€â”€ hr_policy.pdf
 â””â”€â”€ company_overview.txt
```

---

### Step 2: Parsing (Text nikalna)

Backend kya karta hai?

* Har file open karta hai
* Uske andar ka **actual text extract** karta hai
* PDF ka page text, Word ka paragraph text

ğŸ“Œ Output:

```text
"GlobalLogic follows a hybrid work policy..."
```

â— Is stage pe **AI involved nahi hota** â€” sirf text reading.

---

### Step 3: Chunking (Text todna)

Problem:

* Document bahut lamba hota hai
* LLM ek baar me sab nahi padh sakta

Solution:

* Text ko **small pieces (chunks)** me tod diya jaata hai

âš™ï¸ Tumhare config ke according:

```yaml
chunk_size: 500
chunk_overlap: 50
```

ğŸ’¡ Example:

```
Chunk 1 â†’ "GlobalLogic follows hybrid work..."
Chunk 2 â†’ "Employees must be available during core hours..."
```

ğŸ“Œ Overlap isliye hota hai taaki meaning break na ho.

---

### Step 4: Embedding (Meaning ko numbers me badalna)

Ab AI ka first use hota hai ğŸ‘‡

* Har chunk ko **embedding model (nomic-embed-text)** ko diya jaata hai
* Model uska **semantic meaning** nikalta hai
* Meaning ko **numbers (vector)** me convert karta hai

ğŸ“Œ Example:

```
"Hybrid work policy" â†’ [0.12, 0.98, 0.44, ...]
```

â— Ye **LLM nahi**, embedding model hota hai.

---

### Step 5: Vector Storage (ChromaDB)

* Ye vectors + text **Vector DB (Chroma)** me store hote hain
* Har chunk ke saath metadata hota hai:

  * source file
  * page number

ğŸ“¦ Final result:

```
Vector DB = company knowledge ka brain
```

ğŸŸ¢ **Data Ingestion yahin complete ho jaata hai**

---

## ğŸŸ© PART 2: Runtime Query Flow

ğŸ‘‰ *Ye har baar hota hai jab user question poochta hai*

---

### Step 1: User Query

User frontend me type karta hai:

> â€œWhat is the work from home policy?â€

---

### Step 2: Query Embedding

Backend:

* Same embedding model ko **user question** deta hai
* Question ka bhi vector ban jaata hai

ğŸ“Œ Example:

```
"What is the work from home policy?"
â†’ [0.11, 0.97, 0.40, ...]
```

---

### Step 3: Vector DB Similarity Search (Top-K)

Ab magic hota hai ğŸ”¥

* ChromaDB:

  * Question vector
  * Sab stored document vectors
* **Similarity compare karta hai**

ğŸ’¡ Matlab:

> Kaunsa document chunk is question ke meaning ke sabse paas hai?

âš™ï¸ Tumhare config:

```yaml
top_k: 3
```

ğŸ“Œ Output:

```
Top 3 most relevant chunks:
- Chunk about hybrid policy
- Chunk about core working hours
- Chunk about remote approval
```

â— **LLM abhi tak involved nahi hai**

---

### Step 4: Context Assembly (Prompt banana)

Backend ab ek **prompt build** karta hai:

```
Context:
- GlobalLogic follows a hybrid work policy...
- Employees must be available from 11AMâ€“4PM...
- Remote work requires manager approval...

Question:
What is the work from home policy?
```

ğŸ“Œ Is context me **sirf relevant text hota hai**, poora document nahi.

---

### Step 5: LLM Prompting (LLaMA via Ollama)

Ab LLaMA ko kaam diya jaata hai:

* Ollama ke through
* LLaMA ko **sirf ye context + question** milta hai

âš ï¸ Important:

> LLaMA documents ko search nahi karta
> LLaMA sirf **diye gaye context ko read karta hai**

---

### Step 6: Final Answer

LLM output deta hai:

âœ…

> â€œGlobalLogic follows a hybrid work policy where employees work remotely with defined core hours and managerial approval.â€

Agar context me answer nahi hota:

âŒ

> â€œI donâ€™t know.â€

ğŸ“Œ **No hallucination â€” industry best practice**

---

## ğŸ§  Ek Line me Samjho

> **Vector DB batata hai â€œkya padhna haiâ€**
> **LLM batata hai â€œkaise jawab likhna haiâ€**

---

## 5. System Architecture (End-to-End)

### Backend Architecture

```
Dataset (PDF, DOCX, TXT, JSON)
        â†“
Document Loader & Parser
        â†“
Chunker (Recursive / Semantic)
        â†“
Embedding Generator (Ollama)
        â†“
Vector Database (ChromaDB)
        â†“
Retriever
        â†“
LLM (LLaMA via Ollama)
        â†“
Answer to User
```
# ğŸ§© System Architecture (End-to-End) â€“ Simple Explanation

*(How backend actually works in KnowledgeHub AI)*

Is architecture ko samajhne ka easiest tareeqa hai:
ğŸ‘‰ **â€œData andar kaise jaata hai, store hota hai, aur user ko answer kaise milta haiâ€**

## 1ï¸âƒ£ Dataset (PDF, DOCX, TXT, JSON)

ğŸ“‚ Ye tumhara **raw input** hai.

* Company policies
* HR documents
* Guidelines
* Technical docs

ğŸ“Œ Tumhare project me:

* Ye files `data/` folder me hoti hain
* Backend directly yahin se documents uthata hai

Example:

```
data/
 â”œâ”€â”€ hr_policy.pdf
 â”œâ”€â”€ company_overview.txt
```

---

## 2ï¸âƒ£ Document Loader & Parser

ğŸ§  Backend ka first kaam: **files padhna**

* PDF â†’ text nikaala
* Word â†’ paragraphs nikaale
* TXT â†’ direct text

â— Yahan **AI use nahi hota**
Sirf file reading hoti hai.

ğŸ“Œ Output:

```text
"GlobalLogic follows a hybrid work policy..."
```

---

## 3ï¸âƒ£ Chunker (Recursive / Semantic)

Problem:

* Documents bahut bade hote hain
* LLM ek baar me sab nahi padh sakta

Solution:

* Text ko **chhote-chhote pieces (chunks)** me tod diya jaata hai

âš™ï¸ Tumhare config ke according:

```yaml
chunk_size: 500
chunk_overlap: 50
```

ğŸ“Œ Result:

```
Chunk 1 â†’ Hybrid policy
Chunk 2 â†’ Core working hours
Chunk 3 â†’ Remote work approval
```

---

## 4ï¸âƒ£ Embedding Generator (Ollama)

Yahan pe **meaning capture hota hai**

* Har chunk ko **embedding model** diya jaata hai
* Model text ka **semantic meaning** samajhta hai
* Meaning ko numbers (vectors) me convert karta hai

ğŸ“Œ Example:

```
"Hybrid work policy"
â†’ [0.21, 0.89, 0.43, ...]
```

âš ï¸ Ye LLaMA nahi hota
Ye **nomic-embed-text** model hota hai (via Ollama)

---

## 5ï¸âƒ£ Vector Database (ChromaDB)

ğŸ“¦ Ab sab knowledge store hoti hai

* Chunk text
* Uska vector
* Metadata (file name, page no.)

ğŸ“Œ ChromaDB:

* Local
* Fast
* Lightweight
* Tumhare laptop ke liye perfect

ğŸ’¡ Isko samjho:

> **Vector DB = company documents ka brain**

---

## 6ï¸âƒ£ Retriever

ğŸ‘€ Ye component decide karta hai:

> *â€œIs question ke liye kaunsa data relevant hai?â€*

Flow:

* User ka question aata hai
* Uska bhi embedding banta hai
* Vector DB se **Top-K similar chunks** nikale jaate hain

âš™ï¸ Example:

```yaml
top_k: 3
```

ğŸ“Œ Output:

```
3 sabse relevant chunks
```

---

## 7ï¸âƒ£ LLM (LLaMA via Ollama)

ğŸ¤– Ab actual answer writing hoti hai

* Retriever ke chunks ko **context** banaya jaata hai
* Question + context LLaMA ko diya jaata hai

âš ï¸ Important:

> LLaMA documents ko search nahi karta
> Sirf **jo context diya gaya hai wahi padhta hai**

---

## 8ï¸âƒ£ Answer to User

ğŸ¯ Final result frontend pe dikh jaata hai

âœ… Agar answer context me hai:

> â€œGlobalLogic follows a hybrid work policy with defined core hours.â€

âŒ Agar context me nahi hai:

> â€œI donâ€™t know.â€

ğŸ“Œ **No hallucination â€” professional AI behavior**

---

## ğŸ§  One-Line Summary (Interview Ready)

> *In KnowledgeHub AI, documents are converted into embeddings and stored in a vector database. During runtime, relevant content is retrieved using similarity search and passed to a local LLaMA model via Ollama to generate accurate, context-aware answers.*

---

## ğŸ“Œ Simple Analogy (Best for PPT)

* **ChromaDB** â†’ Book index
* **Retriever** â†’ Relevant page finder
* **LLaMA** â†’ Answer writer
* **Ollama** â†’ Local AI engine

---

## 6. Technology Stack & Rationale

### Programming Language

**Python**

* Best ecosystem for RAG
* Strong NLP & AI libraries
* Easy extensibility

---

### Document Loading

Using **LangChain Loaders**:

* PyPDFLoader (PDF)
* TextLoader (TXT)
* JSONLoader (JSON)
* Word Loaders (DOCX)

**Reason:**
They normalize output into `Document(page_content, metadata)` format and handle encoding safely.

---

### Chunking Strategy

* **Chunk Size:** 500 tokens
* **Chunk Overlap:** 50 tokens

**Why Chunking?**

* LLM context window is limited.
* Smaller chunks improve semantic embeddings.
* Overlap ensures contextual continuity.

## ğŸ§  Why Python (and not Node.js) for KnowledgeHub AI?

### 1ï¸âƒ£ Python is the **Industry Standard for AI & RAG**

**Reason:**

* Almost **all AI research + production RAG systems** Python me likhe jaate hain.
* LangChain, ChromaDB, embedding models â€” sab Python-first hain.

ğŸ‘‰ Node.js AI ke liye bana hi nahi tha, wo **web servers** ke liye bana hai.

---

### 2ï¸âƒ£ Best Library Ecosystem (Biggest Reason)

Tumhare project me ye cheezein core hain:

| Requirement                  | Python           | Node.js     |
| ---------------------------- | ---------------- | ----------- |
| Document loaders (PDF, DOCX) | âœ… Excellent      | âŒ Limited   |
| Embeddings & NLP             | âœ… Best-in-class  | âŒ Weak      |
| Vector DB clients            | âœ… Native support | âš ï¸ Partial  |
| LLM integration              | âœ… Direct         | âš ï¸ Indirect |
| AI research tools            | âœ… Massive        | âŒ Minimal   |

ğŸ“Œ Example:

* **LangChain** â†’ Python-first
* **ChromaDB** â†’ Python-first
* **Sentence Transformers** â†’ Python-only

---

### 3ï¸âƒ£ Easier RAG Pipeline Development

RAG flow me heavy processing hoti hai:

* Text parsing
* Chunking
* Vector math
* Similarity search

Python:

* Built for **data processing**
* Clean, readable code
* Less boilerplate

Node.js:

* Async-heavy
* Complex for data pipelines
* Debugging RAG logic becomes messy

---

### 4ï¸âƒ£ Local LLM + Ollama Integration is Better in Python

Tum local LLaMA use kar rahe ho via **Ollama**.

Python:

* Simple HTTP client
* Stable wrappers (langchain-ollama)
* Easy fallback to OpenAI/Gemini later

Node.js:

* Less mature wrappers
* Less community examples
* Debugging harder

---

### 5ï¸âƒ£ Performance Reality (Important Truth)

Myth:
âŒ â€œNode.js is faster than Pythonâ€

Reality:

* **LLM inference time** dominates (not Python/Node)
* Vector search time dominates
* Python speed is **not a bottleneck**

So choosing Node gives **no real performance gain** here.

---

### 6ï¸âƒ£ Scalability & Future-Proofing

Python allows:

* Easy switch to cloud LLMs
* Easy agent frameworks
* Easy research upgrades (rerankers, hybrid search)

Node.js would block:

* Research-level features
* Advanced RAG upgrades
* Agentic systems

---

## ğŸ§  PPT-Friendly One-Liner

> **Python was chosen over Node.js because it is the industry standard for AI and RAG systems, offering superior NLP libraries, native vector database support, and seamless integration with local and cloud-based LLMs.**

---

## ğŸ”¥ Interview Killer Answer (Short)

> *Node.js is great for frontend and APIs, but for AI-heavy workloads like document parsing, embeddings, and retrieval pipelines, Python offers a far richer and more mature ecosystem, making it the correct engineering choice.*

---

### Embedding Model

**nomic-embed-text (via Ollama)**

**Why?**

* Lightweight
* CPU-friendly
* Designed specifically for embeddings
* Free & local

---

### Vector Database

**ChromaDB**

**Why ChromaDB?**

* Local & file-based
* Zero configuration
* Ideal for MVP and offline usage

**Why not others (currently)?**

* FAISS: In-memory only
* Qdrant: Heavier setup
* Pinecone: Paid & cloud-based

---

### Large Language Model

**LLaMA 3.2 (1B) via Ollama**

**Reason for 1B model:**

* System RAM constraint (~2GB)
* Smooth CPU execution
* Sufficient for factual Q&A

Larger models (3B+) and other model apis can be enabled and used later.

---
## 7. Configuration-Driven Design

All system behavior is controlled via `config.yaml`.

### Key Parameters

```yaml
chunk_size: 500
chunk_overlap: 50
vector_db: chroma
embedding_provider: ollama
llm_provider: ollama
```

**Benefits:**

* No hardcoding
* Easy model switching
* Clean separation of concerns

## ğŸ”¹ What is `chunk_size` and `chunk_overlap`?

In KnowledgeHub AI, **documents ko directly LLM ko nahi dete**.
Pehle unhe **small meaningful pieces (chunks)** me todte hain.

---

## ğŸ“Œ `chunk_size: 500`

### ğŸ‘‰ Meaning:

* Har document ko **500 tokens ke blocks** me divide kiya jaata hai
* 1 token â‰ˆ Â¾ word (roughly)

### ğŸ” Example:

Original document:

```
Company follows a hybrid work policy.
Employees must work 3 days from office.
Core hours are 10 AM to 4 PM.
Manager approval is required for full remote work.
```

Chunking:

```
Chunk 1 (500 tokens max):
Hybrid work policy + office days + core hours + approvals
```

### âœ… Why 500?

* LLM context limit hota hai
* Embeddings **best quality** 200â€“800 tokens ke beech aati hain
* 500 = **balanced choice**

  * Not too small (loss of meaning)
  * Not too big (noise)

---

## ğŸ“Œ `chunk_overlap: 50`

### ğŸ‘‰ Meaning:

* Har next chunk me **50 tokens pichhle chunk se repeat** hote hain

### ğŸ” Example:

```
Chunk 1:
"... core hours are 10 AM to 4 PM ..."

Chunk 2:
"10 AM to 4 PM ... manager approval required ..."
```

ğŸ‘‰ â€œ10 AM to 4 PMâ€ dono chunks me aayega

---

## â“ Why Overlap is Needed?

### ğŸš« Without overlap (bad):

* Sentence aadha ek chunk me
* Baaki aadha next chunk me
* Meaning toot jaata hai

### âœ… With overlap (good):

* Context continuity bana rehta hai
* Similarity search better hoti hai
* Answers zyada accurate aate hain

---

## ğŸ§  Real Backend Flow (Tumhare Project Me)

1. PDF read hota hai
2. Text â†’ chunks of 500 tokens
3. Har chunk me 50 tokens overlap
4. Har chunk ka embedding banta hai
5. Vector DB me store hota hai

During query:

* Question embedding banta hai
* Similar chunks retrieve hote hain
* LLM ko **complete context** milta hai

---

## ğŸ§¾ PPT-Ready Explanation (Short)

> **Chunk Size (500)** ensures each text segment is small enough for efficient embeddings while retaining semantic meaning.
> **Chunk Overlap (50)** preserves contextual continuity across chunks, preventing loss of information at boundaries and improving retrieval accuracy.

---

## ğŸ¯ One-Line Analogy (Best for Understanding)

> Chunk size = page size
> Chunk overlap = page ka thoda overlap taaki sentence toot na jaaye

---

## 8. Backend Working (Step-by-Step)

1. Documents are read from the `data/` directory.
2. Text is extracted and split into chunks.
3. Each chunk is converted into an embedding via Ollama.
4. Embeddings are stored in ChromaDB.
5. On user query:

   * Query embedding is generated.
   * Vector DB performs similarity search.
   * Top-K chunks are retrieved.
   * Backend assembles context.
   * LLM generates answer using only provided context.

ğŸ“Œ The LLM **does not search documents directly**.

---

## 9. Role of Ollama in the System

**Ollama acts as a local AI server running on `localhost`.**

* Hosts LLaMA and embedding models.
* Exposes REST APIs for:

  * Embedding generation
  * Text generation

### Benefits

* Complete data privacy
* No internet dependency
* No API cost
* Lightweight and CPU-friendly

---

## 10. Frontend Overview

* Web-based UI (HTML, CSS, JavaScript)
* Dark-themed, premium look
* Chat-style interface
* Communicates with backend via FastAPI

### User Flow

```
User â†’ Ask Question â†’ Backend â†’ Answer Displayed
```

---

## 11. Current Limitations (Intentional)

* Single LLM provider (Ollama) in my project case. Can we use multiple providers in future. 
* No LLM-based intelligent routing (Rule-based routing implemented)

User Question
   â†“
Vector DB Search
   â†“
Context Found?
   â†“ Yes           â†“ No
LLM (Docs)    Web Search â†’ LLM

* Smart web fallback when documents lack answers
* Uses controlled web search
* LLM answers strictly from search context
* Maintains accuracy + expands coverage

These limitations are part of the **MVP scope**.

1ï¸âƒ£ Single LLM Provider (Ollama Only)

What it means:

Abhi system sirf Ollama ke through local LLaMA model use karta hai
OpenAI, Gemini, Claude jaise cloud models abhi connected nahi hain

Why this is intentional:

Project ka focus local + privacy-first AI par hai
Internet dependency avoid ki gayi
Limited RAM (2GB) ke liye safe choice

Simple Example:

Jaise pehle sirf ek engine wali car banana,
multi-engine baad me upgrade hota hai.

Local LLMs (e.g., Ollama) run on your own hardware, ensuring complete privacy, zero API costs, and offline functionality, making them ideal for sensitive data and local prototyping. In contrast, Gemini APIs offer superior, cloud-powered reasoning, massive context windows, and multimodal (image/video/text) capabilities, but require internet connectivity and usage-based payments. 

---

## 12. Web Search Integration (Planned & Implemented Idea)

### Motivation

Some queries fall outside company knowledge.

Example:

> â€œWho is Elon Musk?â€

### Approach

* Detect non-company queries
* Perform web search (DuckDuckGo / Google)
* Summarize results using LLM

---

## 13. Debugging & Quality Improvements

Identified issues:

* Over-restrictive system prompts
* Empty retrieval results
* Vector DB inconsistency

Fixes:

* Full vector DB reset
* Re-ingestion of documents
* Prompt refinement
* Debug logging in retriever and search tools

---

## 14. Upgrade Plan â€“ KnowledgeHub AI v2.0

### Vision: **The Brain (Agentic System)**

The system will evolve into an intelligent agent that can:

* Decide whether to use:

  * Vector DB
  * Web Search
* Support multiple LLM providers
* Provide contextual UI greetings

---

## 15. The Brain â€“ High-Level Flow

```
User Query
   â†“
The Brain (Router)
   â†“
Decision
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Vector DB    â”‚
 â”‚ Web Search   â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
LLM (Multi-provider)
   â†“
Final Answer
```

---

## 16. Multi-Model Support (Future)

Planned providers:

* Ollama (Local)
* OpenAI (GPT-4)
* Gemini
* Claude

Switching controlled via config:

```yaml
llm:
  active_provider: gemini
```

---

## 17. Future Enhancements

* Agent-based routing
* Metadata-based filtering (HR, Finance, Engineering)
* Chat history memory
* UI personalization
* Enterprise-scale ingestion pipelines

---

## 18. Conclusion

KnowledgeHub AI demonstrates a **modern, industry-aligned RAG architecture** that securely leverages internal company knowledge. The project focuses on privacy, modularity, and scalability while maintaining a clear roadmap toward a fully agentic, multi-model AI system.

